{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt update && sudo apt install ffmpeg\n",
    "#!sudo apt-get install libportaudio2 portaudio19-dev\n",
    "#!sudo apt install python3-gi python3-gi-cairo gir1.2-gobject-2.0 gir1.2-gstreamer-1.0 gstreamer1.0-plugins-good gstreamer1.0-plugins-base gstreamer1.0-gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-whisper sounddevice scipy ollama pyttsx3 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114708c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install edge-tts playsound==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63836bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d7917f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting noisereduce\n",
      "  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (from noisereduce) (1.15.2)\n",
      "Collecting matplotlib (from noisereduce)\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from noisereduce) (2.2.4)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from noisereduce) (4.67.1)\n",
      "Collecting joblib (from noisereduce)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->noisereduce)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->noisereduce)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->noisereduce)\n",
      "  Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->noisereduce)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->noisereduce) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib->noisereduce)\n",
      "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->noisereduce)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
      "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, matplotlib, noisereduce\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.57.0 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.1 noisereduce-3.0.3 pillow-11.2.1 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install noisereduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3042e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webrtcvad in ./.venv/lib/python3.10/site-packages (2.0.10)\n",
      "Collecting webrtcvad-wheels\n",
      "  Downloading webrtcvad_wheels-2.0.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Downloading webrtcvad_wheels-2.0.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Installing collected packages: webrtcvad-wheels\n",
      "Successfully installed webrtcvad-wheels-2.0.14\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (78.1.0)\n"
     ]
    }
   ],
   "source": [
    "#!sudo apt install build-essential python3-dev\n",
    "!pip install webrtcvad\n",
    "!pip install webrtcvad-wheels\n",
    "!pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4aacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a5760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafadom/2ºCuatrimestre/TFM-main/test reconocimiento de audio/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import whisper\n",
    "import ollama\n",
    "import edge_tts\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import asyncio\n",
    "import webrtcvad\n",
    "from collections import deque\n",
    "import io\n",
    "import wave\n",
    "import string\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae834a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuración General ---\n",
    "WHISPER_MODEL = \"small\"\n",
    "OLLAMA_MODEL = \"gemma3\" # O el modelo específico que usas\n",
    "TEMP_AUDIO_FILE_TTS = \"temp_tts_output_notebook.mp3\"\n",
    "EXIT_KEYWORDS = [\"adiós\", \"terminar\", \"salir\", \"hasta luego\", \"bye\", \"exit\", \"adios\"]\n",
    "\n",
    "# --- Configuración de Audio y VAD ---\n",
    "VAD_SAMPLE_RATE = 16000\n",
    "VAD_FRAME_MS = 30\n",
    "VAD_CHUNK_SIZE = (VAD_SAMPLE_RATE * VAD_FRAME_MS) // 1000\n",
    "VAD_AGGRESSIVENESS = 1\n",
    "SILENCE_DURATION_S = 0.8\n",
    "MIN_SPEECH_DURATION_S = 0.3\n",
    "PADDING_S = 0.2\n",
    "MICROPHONE_DEVICE_INDEX = None\n",
    "\n",
    "# --- Configuración de Voz TTS (edge-tts) ---\n",
    "TTS_VOICE = \"es-ES-ElviraNeural\"\n",
    "\n",
    "# --- System Prompt (Define la personalidad/instrucciones base) ---\n",
    "# Se enviará con CADA pregunta al LLM, pero sin historial previo.\n",
    "try:\n",
    "    current_time = time.time()\n",
    "    current_date_es = time.strftime('%A, %d de %B de %Y', time.localtime(current_time))\n",
    "    current_location = \"Almería, Andalucía, España\"\n",
    "except Exception:\n",
    "    current_date_es = \"una fecha actual\"\n",
    "    current_location = \"tu ubicación actual\"\n",
    "SYSTEM_PROMPT = f\"Eres un asistente de IA llamado Jarvis. Responde de forma concisa y útil en español de España a la pregunta del usuario. Ignora cualquier conversación anterior. Hoy es {current_date_es}.\"\n",
    "\n",
    "# --- Variables Globales y Evento Async ---\n",
    "audio_buffer = deque()\n",
    "recording_complete_event = asyncio.Event()\n",
    "is_speaking_global = False\n",
    "speech_frames_count_global = 0\n",
    "silence_frames_count_global = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e109501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo Whisper...\n",
      "Modelo Whisper 'small' cargado.\n"
     ]
    }
   ],
   "source": [
    "# --- Inicialización VAD ---\n",
    "try:\n",
    "    vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)\n",
    "except Exception as e:\n",
    "    print(f\"Error inicializando VAD: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Inicialización Whisper ---\n",
    "print(\"Cargando modelo Whisper...\")\n",
    "try:\n",
    "    whisper_model = whisper.load_model(WHISPER_MODEL)\n",
    "    print(f\"Modelo Whisper '{WHISPER_MODEL}' cargado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando el modelo Whisper: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b68651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando dispositivos de audio...\n",
      "   0 HDA NVidia: HDMI 0 (hw:0,3), ALSA (0 in, 8 out)\n",
      "   1 HDA NVidia: HDMI 1 (hw:0,7), ALSA (0 in, 8 out)\n",
      "   2 HDA NVidia: HDMI 2 (hw:0,8), ALSA (0 in, 8 out)\n",
      "   3 HDA NVidia: HDMI 3 (hw:0,9), ALSA (0 in, 8 out)\n",
      "   4 sof-hda-dsp: - (hw:1,0), ALSA (2 in, 2 out)\n",
      "   5 sof-hda-dsp: - (hw:1,3), ALSA (0 in, 2 out)\n",
      "   6 sof-hda-dsp: - (hw:1,4), ALSA (0 in, 2 out)\n",
      "   7 sof-hda-dsp: - (hw:1,5), ALSA (0 in, 2 out)\n",
      "   8 sof-hda-dsp: - (hw:1,6), ALSA (2 in, 0 out)\n",
      "   9 sof-hda-dsp: - (hw:1,7), ALSA (2 in, 0 out)\n",
      "  10 hdmi, ALSA (0 in, 8 out)\n",
      "  11 pulse, ALSA (32 in, 32 out)\n",
      "* 12 default, ALSA (32 in, 32 out)\n",
      "Dispositivo de entrada predeterminado del sistema: 12\n",
      "Usando dispositivo de entrada con índice: 12\n"
     ]
    }
   ],
   "source": [
    "# --- Listar Dispositivos de Audio (para diagnóstico) ---\n",
    "print(\"Buscando dispositivos de audio...\")\n",
    "try:\n",
    "    print(sd.query_devices())\n",
    "    default_input_device = sd.default.device[0]\n",
    "    print(f\"Dispositivo de entrada predeterminado del sistema: {default_input_device}\")\n",
    "    if MICROPHONE_DEVICE_INDEX is None:\n",
    "        MICROPHONE_DEVICE_INDEX = default_input_device\n",
    "    print(f\"Usando dispositivo de entrada con índice: {MICROPHONE_DEVICE_INDEX}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al consultar dispositivos de audio: {e}. Usando predeterminado si es posible.\")\n",
    "    # No salimos, intentaremos continuar con el predeterminado si falla la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c6d11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funciones ---\n",
    "\n",
    "def _audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Callback de sounddevice, procesa audio en tiempo real con VAD.\"\"\"\n",
    "    global audio_buffer, is_speaking_global, speech_frames_count_global, silence_frames_count_global, recording_complete_event\n",
    "    if status: print(f\"Status del stream de audio: {status}\", file=sys.stderr)\n",
    "    clipped_indata = np.clip(indata, -1.0, 1.0)\n",
    "    indata_int16 = (clipped_indata * 32767).astype(np.int16)\n",
    "    audio_chunk_bytes = indata_int16.tobytes()\n",
    "    bytes_per_sample = 2\n",
    "    samples_per_vad_chunk = VAD_CHUNK_SIZE\n",
    "    bytes_per_vad_chunk = samples_per_vad_chunk * bytes_per_sample\n",
    "    for i in range(0, len(audio_chunk_bytes), bytes_per_vad_chunk):\n",
    "        chunk_vad = audio_chunk_bytes[i : i + bytes_per_vad_chunk]\n",
    "        if len(chunk_vad) < bytes_per_vad_chunk: continue\n",
    "        try: is_speech = vad.is_speech(chunk_vad, VAD_SAMPLE_RATE)\n",
    "        except Exception: is_speech = False\n",
    "        if is_speech:\n",
    "            silence_frames_count_global = 0\n",
    "            if not is_speaking_global:\n",
    "                is_speaking_global = True; speech_frames_count_global = 1\n",
    "                padding_frames = int((PADDING_S * VAD_SAMPLE_RATE) / VAD_CHUNK_SIZE)\n",
    "                for _ in range(padding_frames): audio_buffer.append(b'\\x00' * bytes_per_vad_chunk)\n",
    "            audio_buffer.append(chunk_vad); speech_frames_count_global += 1\n",
    "        else:\n",
    "            if is_speaking_global:\n",
    "                silence_frames_count_global += 1; audio_buffer.append(chunk_vad)\n",
    "                silence_threshold_frames = int((SILENCE_DURATION_S * VAD_SAMPLE_RATE) / VAD_CHUNK_SIZE)\n",
    "                if silence_frames_count_global >= silence_threshold_frames:\n",
    "                    is_speaking_global = False\n",
    "                    padding_frames = int((PADDING_S * VAD_SAMPLE_RATE) / VAD_CHUNK_SIZE)\n",
    "                    for _ in range(padding_frames): audio_buffer.append(b'\\x00' * bytes_per_vad_chunk)\n",
    "                    min_speech_frames = int((MIN_SPEECH_DURATION_S * VAD_SAMPLE_RATE) / VAD_CHUNK_SIZE)\n",
    "                    if speech_frames_count_global > min_speech_frames:\n",
    "                        try: loop = asyncio.get_event_loop(); loop.call_soon_threadsafe(recording_complete_event.set)\n",
    "                        except RuntimeError: recording_complete_event.set()\n",
    "                    else: audio_buffer.clear(); speech_frames_count_global = 0\n",
    "                    silence_frames_count_global = 0\n",
    "\n",
    "async def listen_for_speech():\n",
    "    \"\"\"Inicia el stream de audio y espera a que VAD detecte fin de habla.\"\"\"\n",
    "    global audio_buffer, recording_complete_event, is_speaking_global, speech_frames_count_global, silence_frames_count_global\n",
    "    audio_buffer.clear(); recording_complete_event.clear()\n",
    "    is_speaking_global = False; speech_frames_count_global = 0; silence_frames_count_global = 0\n",
    "    print(\"🎤 Escuchando...\", flush=True)\n",
    "    if VAD_SAMPLE_RATE not in [8000, 16000, 32000, 48000]:\n",
    "         print(f\"Error: Sample rate {VAD_SAMPLE_RATE}Hz no soportado por VAD.\", file=sys.stderr); return None\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        stream = sd.InputStream(\n",
    "            samplerate=VAD_SAMPLE_RATE, channels=1, dtype='float32', blocksize=VAD_CHUNK_SIZE,\n",
    "            device=MICROPHONE_DEVICE_INDEX,\n",
    "            callback=lambda indata, frames, time, status: loop.call_soon_threadsafe(_audio_callback, indata, frames, time, status))\n",
    "        with stream: await recording_complete_event.wait()\n",
    "        return b\"\".join(list(audio_buffer))\n",
    "    except sd.PortAudioError as pae:\n",
    "         print(f\"\\nError de PortAudio: {pae}\", file=sys.stderr); print(\"Verifica el micrófono.\", file=sys.stderr)\n",
    "         try: print(sd.query_devices())\n",
    "         except Exception: pass; return None\n",
    "    except Exception as e: print(f\"\\nError en stream de audio: {e}\", file=sys.stderr); return None\n",
    "\n",
    "def save_audio_to_wav_bytes(audio_data: bytes, sample_rate: int) -> bytes:\n",
    "    \"\"\"Guarda los datos de audio (bytes, int16) en formato WAV en memoria.\"\"\"\n",
    "    bytes_io = io.BytesIO()\n",
    "    with wave.open(bytes_io, 'wb') as wf:\n",
    "        wf.setnchannels(1); wf.setsampwidth(2); wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio_data)\n",
    "    return bytes_io.getvalue()\n",
    "\n",
    "async def transcribe_audio_bytes(audio_bytes: bytes, model, sample_rate: int):\n",
    "    \"\"\"Transcribe datos de audio (WAV en bytes) usando Whisper, con reducción de ruido opcional.\"\"\"\n",
    "    if not audio_bytes: return None\n",
    "    print(\"Procesando audio (reducción de ruido y transcripción)...\", flush=True)\n",
    "    try:\n",
    "        # Cargar audio WAV desde bytes\n",
    "        with io.BytesIO(audio_bytes) as wav_buffer:\n",
    "            samplerate_wav, data_wav = wav.read(wav_buffer)\n",
    "\n",
    "        # Convertir a float32 para noisereduce y whisper\n",
    "        audio_np = data_wav.astype(np.float32) / 32768.0\n",
    "\n",
    "        # --- Aplicar Reducción de Ruido ---\n",
    "        # Nota: 'prop_decrease' controla cuánto reducir el ruido, ajustar si es necesario\n",
    "        # 'sr' debe coincidir con la tasa de muestreo del audio\n",
    "        print(\"Aplicando reducción de ruido...\", flush=True)\n",
    "        reduced_noise_audio = await asyncio.to_thread(\n",
    "            nr.reduce_noise,\n",
    "            y=audio_np,\n",
    "            sr=sample_rate,\n",
    "            prop_decrease=0.8 # Reduce el ruido estimado en un 80% (ajustable)\n",
    "        )\n",
    "        print(\"Reducción de ruido completada.\", flush=True)\n",
    "        # ----------------------------------\n",
    "\n",
    "        # Transcribir el audio LIMPIO\n",
    "        print(\"Transcribiendo audio limpio...\", flush=True)\n",
    "        result = await asyncio.to_thread(\n",
    "            model.transcribe,\n",
    "            reduced_noise_audio, # <--- Usar el audio procesado\n",
    "            language='es',\n",
    "            fp16=False\n",
    "        )\n",
    "        transcription = result[\"text\"].strip()\n",
    "        if transcription: print(f\"Texto reconocido: '{transcription}'\", flush=True)\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante reducción de ruido o transcripción: {e}\", flush=True)\n",
    "        return None\n",
    "\n",
    "# --- MODIFICACIÓN CLAVE: Función LLM sin historial ---\n",
    "async def get_llm_response_stateless(prompt, model_name):\n",
    "    \"\"\"Obtiene una respuesta del modelo Ollama tratando cada prompt como independiente.\"\"\"\n",
    "    global SYSTEM_PROMPT # Acceder al prompt del sistema global\n",
    "    print(f\"Pensando (modelo: {model_name})...\", flush=True)\n",
    "    try:\n",
    "        # Crear la lista de mensajes SOLO con el prompt del sistema y el prompt actual\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ]\n",
    "\n",
    "        # Llamar a ollama.chat con esta lista limitada de mensajes\n",
    "        response = await asyncio.to_thread(\n",
    "             ollama.chat,\n",
    "             model=model_name,\n",
    "             messages=messages # <- Solo system + user actual\n",
    "        )\n",
    "\n",
    "        llm_response = response['message']['content']\n",
    "        # Ya NO añadimos nada al historial aquí\n",
    "        print(f\"Respuesta de Ollama: '{llm_response}'\", flush=True)\n",
    "        return llm_response\n",
    "    except Exception as e:\n",
    "        print(f\"Error contactando con Ollama: {e}\", flush=True)\n",
    "        # Ya no hay historial que limpiar\n",
    "        return \"Lo siento, hubo un error al contactar el modelo de lenguaje.\"\n",
    "\n",
    "async def speak_text_edge(text, voice, output_filename):\n",
    "    \"\"\"Genera y reproduce audio TTS usando edge-tts y pydub+sounddevice.\"\"\"\n",
    "    if not text: print(\"TTS: Texto vacío, no hay nada que decir.\"); return\n",
    "    print(\"Generando voz...\", flush=True)\n",
    "    temp_tts_file = output_filename\n",
    "    try:\n",
    "        communicate = edge_tts.Communicate(text, voice, rate=\"+10%\")\n",
    "        await communicate.save(temp_tts_file)\n",
    "        print(\"Hablando...\", flush=True)\n",
    "        def play_audio():\n",
    "            try:\n",
    "                audio = AudioSegment.from_mp3(temp_tts_file)\n",
    "                samples = np.array(audio.get_array_of_samples())\n",
    "                if audio.channels > 1: samples = samples.reshape((-1, audio.channels))\n",
    "                sd.play(samples, audio.frame_rate, blocking=True)\n",
    "            except FileNotFoundError: print(f\"Error Pydub: No se pudo cargar {temp_tts_file}. ¿ffmpeg instalado?\", file=sys.stderr)\n",
    "            except NameError: print(\"Error: 'AudioSegment' no está definido. ¿Falta import?\", file=sys.stderr)\n",
    "            except Exception as play_err: print(f\"Error al reproducir audio: {play_err}\", file=sys.stderr)\n",
    "        await asyncio.to_thread(play_audio)\n",
    "    except edge_tts.exceptions.NoAudioReceived: print(\"Error TTS: No se recibió audio. Verifica conexión/voz.\", file=sys.stderr)\n",
    "    except Exception as e: print(f\"Error en síntesis de voz: {e}\", file=sys.stderr)\n",
    "    finally:\n",
    "        if os.path.exists(temp_tts_file):\n",
    "            try: os.remove(temp_tts_file)\n",
    "            except Exception: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c8e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bucle Principal Asíncrono (Modificado para no usar historial) ---\n",
    "async def main_loop():\n",
    "    # Ya NO inicializamos conversation_history aquí\n",
    "\n",
    "    print(\"\\n--- Asistente NO Conversacional Iniciado (Notebook) ---\")\n",
    "    print(\"Cada pregunta se trata de forma independiente.\")\n",
    "    print(f\"Modelo LLM: {OLLAMA_MODEL}\")\n",
    "    print(f\"Modelo Whisper: {WHISPER_MODEL}\")\n",
    "    print(f\"Voz TTS: {TTS_VOICE}\")\n",
    "    print(f\"Di '{', '.join(EXIT_KEYWORDS)}' para salir.\")\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "    try: await speak_text_edge(\"Hola, ¿en qué puedo ayudarte?\", TTS_VOICE, TEMP_AUDIO_FILE_TTS)\n",
    "    except Exception as initial_speak_error: print(f\"Error en saludo inicial: {initial_speak_error}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            audio_data = await listen_for_speech()\n",
    "            if audio_data is None: print(\"Reintentando escucha...\"); await asyncio.sleep(0.5); continue\n",
    "\n",
    "            wav_bytes = save_audio_to_wav_bytes(audio_data, VAD_SAMPLE_RATE)\n",
    "            user_text = await transcribe_audio_bytes(wav_bytes, whisper_model, VAD_SAMPLE_RATE)\n",
    "\n",
    "            if not user_text: print(\"No se entendió el audio.\"); continue\n",
    "\n",
    "            # --- Lógica de Salida (sin cambios) ---\n",
    "            cleaned_text_for_exit = user_text.lower().strip()\n",
    "            translator = str.maketrans('', '', string.punctuation)\n",
    "            cleaned_text_no_punct = cleaned_text_for_exit.translate(translator).strip()\n",
    "            words_in_text = cleaned_text_no_punct.split()\n",
    "            is_exit = any(keyword in words_in_text for keyword in EXIT_KEYWORDS)\n",
    "\n",
    "            if is_exit:\n",
    "                 print(\"Detectada palabra clave de salida.\")\n",
    "                 await speak_text_edge(\"Entendido. ¡Hasta pronto!\", TTS_VOICE, TEMP_AUDIO_FILE_TTS)\n",
    "                 break # Salir del bucle while\n",
    "\n",
    "            # --- MODIFICACIÓN: Llamar a la función LLM sin historial ---\n",
    "            ai_response = await get_llm_response_stateless(user_text, OLLAMA_MODEL)\n",
    "            # --------------------------------------------------------\n",
    "\n",
    "            await speak_text_edge(ai_response, TTS_VOICE, TEMP_AUDIO_FILE_TTS)\n",
    "\n",
    "        except asyncio.CancelledError: print(\"Bucle principal cancelado.\"); break\n",
    "        except KeyboardInterrupt:\n",
    "             print(\"\\nInterrupción detectada en el bucle. Saliendo...\")\n",
    "             try: await speak_text_edge(\"Detectada interrupción. ¡Adiós!\", TTS_VOICE, TEMP_AUDIO_FILE_TTS)\n",
    "             except: pass\n",
    "             break\n",
    "        except Exception as loop_error:\n",
    "             print(f\"\\nError inesperado en el bucle: {loop_error}\")\n",
    "             try: await speak_text_edge(\"Lo siento, ocurrió un error inesperado.\", TTS_VOICE, TEMP_AUDIO_FILE_TTS)\n",
    "             except: pass\n",
    "             await asyncio.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2897fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Asistente NO Conversacional Iniciado (Notebook) ---\n",
      "Cada pregunta se trata de forma independiente.\n",
      "Modelo LLM: gemma3\n",
      "Modelo Whisper: small\n",
      "Voz TTS: es-ES-ElviraNeural\n",
      "Di 'adiós, terminar, salir, hasta luego, bye, exit, adios' para salir.\n",
      "-------------------------------------------\n",
      "Generando voz...\n",
      "Hablando...\n",
      "🎤 Escuchando...\n",
      "Bucle principal cancelado.\n",
      "--- Asistente Conversacional (Notebook) Terminado ---\n"
     ]
    }
   ],
   "source": [
    "# En esta nueva celda, ejecuta esto para iniciar:\n",
    "try:\n",
    "    await main_loop()\n",
    "except (KeyboardInterrupt, asyncio.CancelledError):\n",
    "    print(\"\\nEjecución interrumpida por el usuario.\")\n",
    "finally:\n",
    "    # Limpieza final\n",
    "    if os.path.exists(TEMP_AUDIO_FILE_TTS):\n",
    "        try: os.remove(TEMP_AUDIO_FILE_TTS)\n",
    "        except Exception: pass\n",
    "    print(\"--- Asistente Conversacional (Notebook) Terminado ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
